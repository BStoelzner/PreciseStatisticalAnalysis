{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3129703-56ce-48bc-b0b6-253b67e4bbc5",
   "metadata": {},
   "source": [
    "# Consistency and tension\n",
    "Aim: Determine the level of consistency between two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806d1bf-abe8-432b-9097-391aa341ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "from chainconsumer import ChainConsumer\n",
    "# Sampler\n",
    "import pymultinest\n",
    "# If you haven't installed MultiNest: use conda install -c conda-forge pymultinest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8374ed-ba09-4511-8c76-391a207ea1be",
   "metadata": {},
   "source": [
    "In this tutorial we will work with some mock data resembling a cosmic shear measurement. In cosmic shear we measure the galaxy shapes in several tomographic redshift bins. From these measurements we calculate two-point statistics, from which we then constrain cosmological parameters. \n",
    "\n",
    "Here, we will work with one single redshift bins and pretend that we have two cosmic shear measurements in this bin. Our goal is to asses whether or not the two observations are consistent with each other and if there exists a model that provides a good fit to both datasets at the same time. For simplicity, we will not model the full cosmic shear power spectrum, but instead adopt a very simple model.\n",
    "\n",
    "There are two sets of data in this directory (data_1.txt and data_2.txt). Each file contains three columns: 8 logarithmic $\\ell$-values and two corresponding measurements of cosmic shear band power spectra $C_E$ for dataset A and B. As we will see later, the measurements in one file are consistent with each other while the other ones show some tension. For simplicity, we assume all measurements share the covariance matrix in covariance.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5365d79-5992-429f-a1b2-5c367c461f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in data_1.txt and the covariance matrix in covariance_1.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b63b3-5cfc-444d-bcc1-161e6d4c87ad",
   "metadata": {},
   "source": [
    "Now, plot the two data vectors using matplotlib (including errorbars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91062131-dcc8-414b-8feb-777aceb51c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two datavectors (note: plot ell vs data/ell for better visualisation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6952aa19-3b69-455d-9d90-c24721653e30",
   "metadata": {},
   "source": [
    "Our goal is to fit a theory model to the two observed datasets and quantify the level of consistency between the two experiments. We will use MultiNest to sample the posterior and to infer the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7801423-b816-4f00-8c96-e5ebc39c73b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear model simple model \n",
    "def model(ell, a, b):\n",
    "    return((b + ell*a)*1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef1a31-9108-45f3-be5e-b2210f637bce",
   "metadata": {},
   "source": [
    "MultiNest generates samples in a unit hyper-cube in which all the parameter are uniformly distributed in the interval [0, 1].\n",
    "Therefore, we need to define a function that transforms the interval [0,1] onto our desired prior. Additionally, we need to define the likelihood function.\n",
    "\n",
    "To get started, define a uniform prior in the interval [-2.5,9] for the amplitude and [0.022,0.055] for the tilt.\n",
    "\n",
    "The MultiNest prior function and the likelihood function require three inputs: the MultiNest hypercube, the number of dimensions, and the number of parameters.\n",
    "The number of parameters and the number of dimension are the same, unless we want to generate a set of derived parameters. \n",
    "\n",
    "The prior function should return the transformed hypercube and the likelihood function should return the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8aaf22-e4f6-4b31-8ce0-12e676592ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prior function\n",
    "# The prior function should use the following inputs:\n",
    "# cube: A N-dimension array (where N is the number of model parameters). The entries of this array are the parameter values\n",
    "#       generated by MultiNest during sampling. However, MultiNest generates its parameter values in the interval [0,1],\n",
    "#       which is why we need to define a transformation onto our prior interval. \n",
    "# ndim: The number of model parameters\n",
    "# nparams: The total number of parameters (which can be larger than the number of model parameters if we calculate\n",
    "#          some derived parameters). In this tutorial this parameter will be equal to ndim.\n",
    "# The output of the function should be the transformed N-dimensional array\n",
    "def prior(cube, ndim):\n",
    "    # define a linear transformation from [0,1] to the prior interval\n",
    "\n",
    "    return(transformed_cube)\n",
    "    \n",
    "# Define the likelihood function (hint: use a Gaussian chi^2)\n",
    "# The likelihood function should use the following inputs:\n",
    "# cube: A N-dimensional array, which contains the values of our model parameters for a given point.\n",
    "#       (Internally, MultiNest first generates parameters in the interval [0,1], which are passed to the prior \n",
    "#       function (defined above). The output of the prior function (i.e. the transformed parameter values) is then\n",
    "#       passed to the likelihood function.\n",
    "# ndim: same as above\n",
    "# nparams: same as above\n",
    "\n",
    "def log_lkl(cube, ndim, nparams):\n",
    "    # Define a Gaussian log-likelihood with the data that you loaded from the text file\n",
    "\n",
    "    # return the log-likelihood\n",
    "    return(loglkl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af1a62-a3ec-425c-b2d1-7f8612951e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiNest parameters\n",
    "eff = 0.1\n",
    "tol = 0.001\n",
    "nLive = 1000\n",
    "nDims = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ce143-0cda-46ae-832f-4a1afce77315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the likelihood using multinest\n",
    "# Hint: use pymultinest.run (you can run 'help(pymultinest.run)' to check how to run the sampler.\n",
    "# MultiNest automatically saves some output files to disk. You need to set outputfiles_basename for each chain.\n",
    "\n",
    "# IMPORTANT NOTE: If the output folder doesn't exist, MultiNest WILL CRASH! \n",
    "#                 Make sure to create output folder before running MultiNest!\n",
    "\n",
    "# Additionally, MultiNest requires you to set the following parameters: n_live_points, sampling_efficiency, evidence_tolerance.\n",
    "# Use the values provided in the cell above\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf3739-5766-4d2e-9746-c230e9ee1f73",
   "metadata": {},
   "source": [
    "Now plot the chains using chainconsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac098978-3267-446a-b484-7e50d42191b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the chains\n",
    "# The chains contain 2+nparams columns: weight, -2*loglikehood, parameter values\n",
    "# Hint: pymultinest.Analyzer allows you to access the MultiNest output. Alternatively, you can directly load the chain from outputfiles_basename.txt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ebaf2-bab5-4909-b307-e771d5b6fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the prior, run another chain with constant likelihood to generate samples from the prior\n",
    "# Let's just set up a dummy likelihood function that just return a constant value.\n",
    "\n",
    "# Run MultiNest with this sampler to generate a chain that resembles our prior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b5bff-479d-43f8-892b-61074d690aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the chains, including the prior chain\n",
    "# Hint: Use chainconsumer (You can look up the syntax in the session 1 notebook)\n",
    "# Note: Each point in the MultiNest chain carries a corresponding weight. These need to be passed to chainconsumer as additional input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713da8d-d1e1-4e09-91f9-386bb5b21ce1",
   "metadata": {},
   "source": [
    "It is useful to plot the translated posterior distribution (TPD) to visualize the range of theory vectors in the chain. This requires us to translate each sample in the chain back into the data domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a8231-03d6-4f79-8b9f-063efc27c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each chain, evaluate the model at every point \n",
    "# The result should be a (N_points, N_data)-dimensional array, where N_points is the number of points in the chain and N_data is the number of data points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f6cef-54a1-4019-a132-b9314dce8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data vector together with the mean and the standard deviation of the of the TPD. \n",
    "# Hint: Remember to take the weights into account when calculating the mean and standard deviation.\n",
    "# Use plt.fill_between to plot the 1-sigma interval of the TPD\n",
    "\n",
    "# You can use this package to calculate a weighted mean and standard deviation\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "\n",
    "# Alternatively, you can use weights in np.average. However, np.std doesn't support weighted samples. You could try to calculate the standard deviation by yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fad8d2-af6e-4c9c-9835-5c49d55eb02c",
   "metadata": {},
   "source": [
    "Now, we want to test if the two datasets are consistent with each other. To do so, we test the following hypotheses:\n",
    "1) There exist two sets of parameters that each describe one dataset independently\n",
    "2) There exists one single set of parameters that describes both datasets A and B at the same time\n",
    "\n",
    "So far, our chains have tested hypothesis 2). Next, we will run chains testing hypothesis 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b528a7-0582-4d6e-8aa8-e6e2ad54291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the likelihood function for the combination of datasets A and B (hint: log-likelihoods are additive)\n",
    "def log_lkl(cube, ndim, nparams):\n",
    "\n",
    "    return(loglkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b04c5e-9f95-4675-b8f9-d210edadb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the combined likelihood with a single set of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f36d9-6329-47f6-9ee9-ea0e2dda04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77781e8d-d5a7-489c-84af-04ef9497c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90468bd-5978-4969-8678-5f03d6c3064f",
   "metadata": {},
   "source": [
    "Now that we have run all chains we can make a model comparison to infer which of the two hypotheses is preferred by the data.\n",
    "We calculate the Bayes Ratio:\n",
    "\n",
    "$\\log R = \\log Z_{AB} - \\log Z_{A} - \\log Z_{B}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1d0ca-b8bf-4e23-8988-e07497c15129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Bayes ratio \n",
    "# Hint: Use the get_stats() function of the pymultinest.Analyzer class to get the evidence. \n",
    "# Alternatively, you can take a look at the output files produced by MultiNest\n",
    "\n",
    "print('logR = %.2f'%logR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e10f9-ad84-4b45-b857-de69ab5c918e",
   "metadata": {},
   "source": [
    "Based on the Bayes ratios, in which case are the two datasets consistent with each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad9966-7b34-44d2-ad9b-9e0b2b1316eb",
   "metadata": {},
   "source": [
    "Now we want to investigate the impact of the prior volume on the Bayes ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e3019-cd23-41bd-8de8-25158a1665cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few sets of priors. Take a look at the posterior plot that you produced earlier and define some new priors that are broader/narrower.\n",
    "def prior_wide(cube, ndim, nparams):\n",
    "    \n",
    "    return cube\n",
    "# Define as many priors as you want!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937aca0c-9072-4158-966e-bbd156a0c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the likelihoods with different priors\n",
    "# Individual likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123d251-950e-4c44-9eab-23dd45730c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the new chains\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d2b97-0b35-4b5b-b133-9abfcfc2ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the Bayes ratio for each of your priors. How do the values change for different prior ranges?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072d855-a95b-4ced-8a1b-1a3aa88facbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
