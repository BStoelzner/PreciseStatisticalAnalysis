{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586432b9",
   "metadata": {},
   "source": [
    "# Make your own mock data and write your own MCMC\n",
    "\n",
    "Aim: Understand and write a simple MCMC code that can sample a likelihood surface, then plot results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31447e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "# Genral imports\n",
    "import numpy as np\n",
    "\n",
    "# Plotting imports\n",
    "import matplotlib.pylab as plt\n",
    "from chainconsumer import ChainConsumer\n",
    "from matplotlib import colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97a57aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a list of nice colours for plotting.\n",
    "# If you uncomment the print statement you can see the full list of colour names and define your own.\n",
    "\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "# Sort colors by hue, saturation, value and name.\n",
    "by_hsv = sorted((tuple(mcolors.rgb_to_hsv(mcolors.to_rgba(color)[:3])), name)\n",
    "                for name, color in colors.items())\n",
    "sorted_names = [name for hsv, name in by_hsv]\n",
    "# print(sorted_names)\n",
    "\n",
    "# pick the colours that you want\n",
    "orange = colors['darkorange']\n",
    "pink   = colors['hotpink']\n",
    "blue   = colors['darkturquoise']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75ee8e",
   "metadata": {},
   "source": [
    "## A: Create a data vector\n",
    "\n",
    "First step of the assignment is to create a toy data vector. We start by using this simple function. You can play with other functions later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deb4a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function with two variables\n",
    "def mockdata(x,a,b):\n",
    "    return a*np.log10(x)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5f85a",
   "metadata": {},
   "source": [
    "Now define 9 $x$ values between 0.5 and 300.0 in log10 space. We are going to set $a$ and $b$ to the values below for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8867d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 9 x values between 0.5 and 300 in logspace\n",
    "\n",
    "# Set the input a and b values\n",
    "a = 0.75\n",
    "b = 0.3\n",
    "\n",
    "# Create a mock data vector,call it y_th using the simple function: mockdata for the above x values \n",
    "# y_th = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33d2ae",
   "metadata": {},
   "source": [
    "Now plot your theoretical mock data vector using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ee59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot y_th versus x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9c824",
   "metadata": {},
   "source": [
    "## B: Add noise to mock data\n",
    "We want to make a data vector that resembles real data. Real data has noise, so lets add noise to our theoretical model. \n",
    "\n",
    "### B.1: Add independent noise\n",
    "\n",
    "For simplicity assume:\n",
    "1. The error on each data point is independant of the others. \n",
    "2. All 1$\\sigma$ errors are equal to 1.\n",
    "\n",
    "Now create a noise realisation by randomly picking from a Gaussian distribution (hint: you can use np.random.normal to do this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e49408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noise realisation Gaussian independent error\n",
    "# noise_realisation = \n",
    "\n",
    "# Add noise to the theoretical data vector\n",
    "# y_data = y_th + noise_realisation\n",
    "\n",
    "# plot y_th and y_data (with errorbars, hint: you can use plt.errorbar) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3e06e",
   "metadata": {},
   "source": [
    "### B.2: Add correlated noise\n",
    "In practice the error on our data points may not be independent. Sometimes there is significant correlation between them. Lets define a covariance matrix and create our noise realisation from a multivariate Gaussian distribution. You can use np.random.multivariate_normal, but I recommend using a cholesky decomposition in combination with the method in B.1 to gain some understanding of how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e68b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is your starting covariance matrix. We still have one along the diagonals, \n",
    "# which is the same as what we had in B.1. But now we have a few non-zero off-diagonals.\n",
    "\n",
    "cov = np.array([[1., 0.4, 0., 0., 0., 0., 0., 0., 0.],\n",
    "                [0.4, 1., 0.5, 0., 0., 0., 0., 0., 0.],\n",
    "                [0., 0.5, 1., 0., 0., 0., 0., 0., 0.],\n",
    "                [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "                [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "                [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "                [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "                [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "                [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
    "\n",
    "\n",
    "# Create a noise realisation using this covariance. \n",
    "# look up the cholesky decomposition and use the two line solution (hint: use np.linalg.cholesky)\n",
    "# cholesky_mat = \n",
    "# correlated_noise_realisation =\n",
    "\n",
    "\n",
    "# Add noise to y_th\n",
    "# y_data = y_th + correlated_noise_realisation\n",
    "\n",
    "# plot y_data and y_th again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91bed8e",
   "metadata": {},
   "source": [
    "## C: Fit the model to the data\n",
    "\n",
    "In this example we know the true model and the value of its parameters (we defined it ourselves!). In reality we won't know that. So for now lets forget that we already have this information and try to find a fit to our data.\n",
    "\n",
    "\n",
    "### C.1: Define $\\chi^2$\n",
    "\n",
    "The $\\chi^2$ function for a general case is \n",
    "\n",
    "\\begin{equation}\n",
    "\\chi^2=\\sum_{i,j}^{N}[y_i-\\mu_i(\\Phi)](C^{-1})_{ij}[y_j-\\mu_j(\\Phi)]\\;,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_i$ are the expected values of the observables, $y_i$, \n",
    "and $C^{-1}$ is the inverse covariance matrix. \n",
    "The $\\chi^2$ fitting is a generalized (weighted) form of least-square fitting.\n",
    "To see why notice that if the covariance is a diagonal matrix with equal diagonal components , \n",
    "$\\sigma^2$ (like we had in B.1), the simple least-square formula is attained,\n",
    "\n",
    "\\begin{equation}\n",
    " \\chi^2_\\mathrm{simple}=\\frac{1}{\\sigma^2}\\sum_{i}^{N}[y_i-\\mu_i(\\Phi)]^2\\;,\n",
    "\\end{equation}\n",
    "\n",
    "and minimizing this simplified $\\chi^2$ results in finding the best fitting values.\n",
    "\n",
    "Notice that the general definition can be simplied using matrix multiplication:\n",
    "\n",
    "\\begin{equation}\n",
    "\\chi^2=\\Delta \\mathbf{y}\\; C^{-1}\\; \\Delta \\mathbf{y}^t \\;,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Delta \\mathbf{y} = \\mathbf{y}-\\mathbf{\\mu}$ is a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38d6eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a chi^2 function based on the last equation above.\n",
    "\n",
    "#def chisq_func(...):\n",
    "    #...\n",
    "    #return chisq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f178a",
   "metadata": {},
   "source": [
    "Now find the $\\chi^2$ value for your mock noisy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05702ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chisq = chisq_func(...)\n",
    "# print(chisq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53ee9a",
   "metadata": {},
   "source": [
    "## C.2: Likelihood function\n",
    "\n",
    "We usually assume that our likelihood function is a (multivariate-) Gaussian. In this case it is fully defined by our $\\chi^2$ value. In fact we don't need to define anything else to start sampling the likelihood. \n",
    "\n",
    "A multivariate Gaussian distribution for $\\mathbf{y}$ has the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{likelihood}\n",
    "L(\\boldsymbol{\\mu}(\\Phi)|\\mathbf{y})=\\frac{\\mathrm{e}^{-\\chi^2/2}}{(2\\pi)^{N/2}\\sqrt{\\det C}}\\;,\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of data points.\n",
    "The likelihood in general will not be Gaussian in parameter space. \n",
    "Nevertheless, it is conventional and benign to assume Gaussianity \n",
    "near the maximum of the likelihood in most realistic cases. \n",
    "\n",
    "We also define the log-likelihood function, $\\mathfrak{L}$, as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "2\\mathfrak{L}(\\boldsymbol{\\mu}(\\Phi)|\\mathbf{y}) = \n",
    "\\ln (\\det C)+ N\\ln(2\\pi) + (\\mathbf{y}-\\boldsymbol{\\mu}(\\Phi))^\\mathrm{T}C^{-1}\n",
    "(\\mathbf{y}-\\boldsymbol{\\mu}(\\Phi))\\;.\n",
    "\\end{equation} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938cd43",
   "metadata": {},
   "source": [
    "## C.3: MCMC, Monte Carlo Markov Chain\n",
    "\n",
    "\n",
    "A Markov chain is a random process where each state (point) in the chain depends only on the previous one. \n",
    "For an MCMC chain we need to first pick a start point, $\\phi_{\\rm start}$. \n",
    "Then the next point, $\\phi_{\\rm prop}$, is chosen from a proposal distribution, \n",
    "$P(\\phi_{\\rm prop}|\\phi_{\\rm start})$ which depends on the start point. \n",
    "The proposal distribution can have any functional \n",
    "form and generally can be asymmetric.\n",
    "\n",
    "When the general form of the likelihood distribution is not known beforehand,\n",
    "a practical choice for the proposal distribution is a multivariate Gaussian distribution with the \n",
    "Fisher matrix as the inverse covariance matrix.\n",
    "\n",
    "After picking a point randomly from the proposal distribution its likelihood is compared to the \n",
    "likelihood of the start point. If it is higher then the proposed point is added to the chain, \n",
    "however, a lower likelihood does not automatically exclude the new point. \n",
    "Instead it is accepted by a probability of \n",
    "\n",
    "\\begin{equation}\n",
    " P(\\rm acceptance)= \\frac{L(\\phi_{\\rm prop})P(\\phi_{\\rm prop}|\\phi_{\\rm start})}\n",
    " {L(\\phi_{\\rm start})P(\\phi_{\\rm start}|\\phi_{\\rm prop})}\\;,\n",
    "\\end{equation}\n",
    "\n",
    "where $L(\\phi)$ is the likelihood of the data belonging to the model with parameters $\\phi$. \n",
    "For simplicity most proposal distributions take a symmetric form, i.e. \n",
    "$P(\\phi_{\\rm prop}|\\phi_{\\rm start})=P(\\phi_{\\rm start}|\\phi_{\\rm prop})$.\n",
    "In this case the acceptance probability reduces to the ratio of the likelihood of the proposed point\n",
    "to the start point. If the proposed point is not accepted the start point is repeated in the chain. \n",
    "This process is repeated with each point, that is added to the chain, as the new start point. \n",
    "\n",
    "If the start point of a chain is outside the high likelihood volume a number of steps are needed \n",
    "for the chain to burn into this volume. These burn in points need to be excluded from the analysis, \n",
    "since they depend on the choice of the start point rather than the likelihood itself. \n",
    "Typically, a visual inspection is sufficient for finding the burn in chain. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce1b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a step by step algorithm (in writting first) \n",
    "# that takes the information above and puts it into an itemise list. I have added the first and the last steps:\n",
    "\n",
    "# 1. Choose a starting position in your parameter space (a_start and b_start)\n",
    "# ...\n",
    "# n. Once you have your chain, remember to burn in the starting values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e2c8b",
   "metadata": {},
   "source": [
    "### C.4: Create your proposal distribution\n",
    "\n",
    "Now that you have itemised the steps, you can write your own MCMC code! To create the proposal lets use the recommended approach of using a Fisher matrix. \n",
    "\n",
    "### What is a Fisher Matrix?\n",
    "\n",
    "Consider a model with a number of parameters, $\\phi_j$, \n",
    "and a data set with a number of data points, $y_i$, \n",
    "then Fisher analysis determines the amount of information contained in the data set, \n",
    "$y_i$, about the parameters, $\\phi_j$, assuming that the likelihood is Gaussian. \n",
    "In general this assumption is satisfied near the maximum of the likelihood function. \n",
    "Another way of looking at the Fisher matrix is to acknowledge \n",
    "its close relation to the covariance matrix of the model parameters; \n",
    "in this sense Fisher matrix analysis is a compact way of propagating \n",
    "errors from the observable, to the parameter space.\n",
    "\n",
    "Formally, the Fisher matrix is defined as the ensemble average of second derivatives \n",
    "of log-likelihood function at the maximum likelihood point,\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{fisher}\n",
    "F_{ij}\\equiv \\bigg\\langle\\frac{\\partial^2 \\mathfrak{L}}{\\partial \\phi_i \\: \\partial \\phi_j} \n",
    "\\bigg\\rangle\\;\n",
    "\\end{equation}\n",
    "\n",
    "To make the connection between this definition and the meaning of Fisher matrix explained above \n",
    "let us consider the Taylor expansion of $\\mathfrak{L}_{,\\phi_i}$, where $_{,\\phi_i}$ denotes a partial derivate with respect to the parameter $\\phi_i$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathfrak{L}_{,\\phi_i}(\\bar{\\Phi}|\\mathbf{x})\n",
    "=\\mathfrak{L}_{,\\phi_i}(\\Phi^{(0)}|\\mathbf{x})+\\sum_j\\mathfrak{L}_{,\\phi_i\\phi_j}\n",
    "(\\Phi^{(0)}|\\mathbf{x})(\\bar{\\phi}_j-\\phi^{(0)}_j)+...\\;,\n",
    "%=\\mathfrak{L}_{,\\phi_i}(\\Phi^{(0)})+\\mathrm{D}\\mathfrak{L}_{,\\phi_i}(\\Phi^{(0)})\\:.(\\bar{\\Phi}-\\Phi^{(0)})+...\\;,\n",
    "\\end{equation} \n",
    "\n",
    "The second term in this expansion contains the curvature matrix of the log-likelihood function,\n",
    "$\\mathrm{D}^2\\mathfrak{L}$, which is a measure of how fast the likelihood function \n",
    "drops near its maximum and the Fisher matrix is the expectation value of this curvature.\n",
    "\n",
    "To calculate the Fisher matrix one does not need to find the likelihood function, \n",
    "instead by considering the derivatives of $\\mathfrak{L}$ and their expectation value the following analytical formula is found,\n",
    "\n",
    "\\begin{align}\n",
    "\\label{FisherTeg}\n",
    "F_{ij}=\\langle \\mathfrak{L}_{,ij} \\rangle = \\frac{\\partial \\mathbf{y}}{\\partial \\phi_{i}} \\:C^{-1}\\: \\frac{\\partial \\mathbf{y}}{\\partial \\phi_{j}},\n",
    "\\end{align}\n",
    "\n",
    "where we have assumed that the covariance matrix does vary (significantly) with parameters.\n",
    "From this equations we can see that a model with 2 parameters (like our starting mockdata functions) will have a $2\\times2$ Fisher matrix. Note that the Fisher matrix is always symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2e36517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multivariate Guassian distribution as your proposal distribution\n",
    "\n",
    "# Use the last equation above to estimate your Fisher matrix.\n",
    "# You will need to calculate the derivates of y with respect to your model parameters. \n",
    "# You can write a general function that calculates derivates for any input function.\n",
    "# Or for now write a specific function for your case.\n",
    "\n",
    "# def der(...):\n",
    "#     #...\n",
    "#     return derivative\n",
    "\n",
    "# Now define your Fisher matrix\n",
    "\n",
    "# def Fisher(derivative,cov):\n",
    "#     return Fisher_matrix\n",
    "\n",
    "# Calculate your Fisher matrix\n",
    "# Fisher_mat = Fisher(...)\n",
    "\n",
    "# Create your proposal distribution and plot it using chainconsumer: https://samreay.github.io/ChainConsumer/ \n",
    "# proposal = \n",
    "\n",
    "# Example plotting code snippet\n",
    "# from chainconsumer import ChainConsumer\n",
    "# c = ChainConsumer()\n",
    "# c.add_chain(proposal,color=orange,parameters=['a','b'], kde=1.0,shade=True,name=r\"Proposal\",shade_alpha=0.8)\n",
    "# c.configure(plot_hists=True,shade_gradient=1.0,diagonal_tick_labels=False,label_font_size=14,tick_font_size=13,serif=True,legend_color_text=True,linewidths=1.5,statistics=\"max\")\n",
    "# c.plotter.plot(filename=\"proposal.png\", figsize=1.5, truth=[a,b])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c362c",
   "metadata": {},
   "source": [
    "### C.5: Create your chain and plot the results\n",
    "\n",
    "Now that you have your proposal distribution defined, go through the steps that you have identified above and write your MCMC code. You need to also tell it when to stop. For now lets use a variable called \"nSteps\" to set a fixed number of steps after which we stop the chain. How does this number and your starting parameter values effect the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3376e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your MCMC code and run it on y_data\n",
    "\n",
    "# nSteps = \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0ae23",
   "metadata": {},
   "source": [
    "Now plot your results, use the example given in the proposal section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "09b26e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use chainconsumer to plot your chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3c748",
   "metadata": {},
   "source": [
    "## D: Extras!\n",
    "\n",
    "Congradulations! You have written a working MCMC code (if not go back and try to finish that before continuing). Here are a list of other things to do and think about:\n",
    "\n",
    "1. You saw that we need to tell the chain when to stop. How do we know if the results are stable? This is usually a question of convergence: has my chain converged to something close to the true likelihood distribution? There are a few method for testing convergence. Some are already coded in chainconsumer. Look for them and use them on your chain.\n",
    "2. We haven't talked about burn-in. Depending on where you set your starting point you'll need a different burn-in. Plot your chain steps to get an idea of where that should be. Try different starting points.\n",
    "3. We used a noisy data vector for our test and you might have noticed that the middle of your contours doesn't quite sit on top of the \"True\" values for $a$ and $b$. What if you were to run this chain on the different noise realisation? Think: What would happen if you made many noise realisation and looked at all of their results? How would the contour look like?\n",
    "3. What would happen if you ran the chain on a noiseless data vector, $y_th$?\n",
    "4. We have a very simple model with only two parameters. Our data vector is also linearly dependent on the parameters. What happens if you generalise the model? \n",
    "5. Have you compared your proposal distribution with your chain result? Does it matter what kind of model you have?\n",
    "6. One of our aims when analysing data is to find the best fitting parameters and model. Can you modify your MCMC code so that it finds this value? \n",
    "7. Plot your chains and bestfit values. Compare them to your input values. \n",
    "8. We haven't touched on priors. How would you incorporate priors in your algorithm? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043e2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
